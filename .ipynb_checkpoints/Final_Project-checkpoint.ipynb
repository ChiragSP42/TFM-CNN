{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964d5cf0-2d8e-45bf-99ad-61caf9c19a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matlab.engine\n",
    "import h5py\n",
    "import pymatreader\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "# from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2335f-1ca9-40c9-a72a-05d5b1cd1625",
   "metadata": {},
   "source": [
    "# Running MATLAB code to generate simulated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5d06ea-f4f8-4db4-bea7-f03625f16b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED_IMAGES_FOLDER = 'Simulated Images/'\n",
    "# ROOT_DIR = os.getcwd()\n",
    "# SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "# if os.listdir(SIMULATED_IMAGES_PATH):\n",
    "#     # Start MATLAB engine\n",
    "#     eng = matlab.engine.start_matlab()\n",
    "#     # Specify the root folder containing your MATLAB functions\n",
    "#     # root_folder = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Image Simulation Code/'\n",
    "#     # Generate path string including all subfolders\n",
    "#     # path_string = eng.genpath(root_folder)\n",
    "    \n",
    "#     # Add the generated path to MATLAB search path\n",
    "#     # eng.addpath(path_string, nargout=0)\n",
    "#     nExp = 20;\n",
    "#     force_start = 200.0;\n",
    "#     force_step = 200.0;\n",
    "#     force_stop = 4000.0;\n",
    "#     distance_start = 2.0;\n",
    "#     distance_step = 2.0;\n",
    "#     distance_stop = 20.0;\n",
    "#     storePath = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Simulated Images/'\n",
    "    \n",
    "#     eng.CSPrunTestSingleForcePython(nExp, \n",
    "#                                     force_start, \n",
    "#                                     force_step, \n",
    "#                                     force_stop, \n",
    "#                                     distance_start, \n",
    "#                                     distance_step,\n",
    "#                                     distance_stop,\n",
    "#                                     storePath, \n",
    "#                                     nargout = 0)\n",
    "    \n",
    "#     eng.quit()\n",
    "# else:\n",
    "#     print(\"Simulated Images already present, either delete previous data or provide new path to empty directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50d8fb-ab7d-4975-9890-37528a88bb46",
   "metadata": {},
   "source": [
    "# Get all directories in 'Simulated Images' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a8d62d-af99-4b75-9c7f-18a56bf2331f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_directories(directory_path):\n",
    "    # Get all directories in the specified path\n",
    "    directories = [d for d in Path(directory_path).iterdir() if d.is_dir()]\n",
    "    \n",
    "    # Sort directories by modification time\n",
    "    sorted_directories = sorted(directories, key=lambda x: x.stat().st_mtime, reverse=False)\n",
    "    \n",
    "    return sorted_directories\n",
    "\n",
    "# Example usage\n",
    "SIMULATED_IMAGES_FOLDER = 'Simulated Images'\n",
    "ROOT_DIR = os.getcwd()\n",
    "SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "sorted_dirs = get_sorted_directories(SIMULATED_IMAGES_PATH)\n",
    "\n",
    "# for directory in sorted_dirs:\n",
    "#     print(f\"{directory.name}: {os.path.getmtime(directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb34a6b-e5ec-4913-a682-1276d515c753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc6c4d-8dbe-4eb3-84b7-190b693ed83e",
   "metadata": {},
   "source": [
    "# Extract all images, and force vectors from all directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed187e5b-050e-4286-8b85-8ff30557930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files\n",
      "Final dimensions:\n",
      "Number of reference images: 4000 of shape: (256, 256)\n",
      "Number of bead images: 4000 of shape: (256, 256)\n",
      "Force in X-direction: (4000, 256, 256)\n",
      "Force in Y-direction: (4000, 256, 256)\n",
      "Dimension of images: (4000, 2, 256, 256)\n",
      "Dimension of forces: (4000, 2, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "np_files = ['bead_images.npy', 'ref_images.npy', 'force_x.npy', 'force_y.npy', 'images.npy', 'forces.npy']\n",
    "all_present = True\n",
    "\n",
    "for file in np_files:\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), file)):\n",
    "        print(f\"{file} does not exist\")\n",
    "        all_present = False\n",
    "        \n",
    "if not all_present:\n",
    "    REF_IMAGE_FILE = 'Reference/img1ref.tif'\n",
    "    BEAD_IMAGE_FILE = 'Beads/img2bead.tif'\n",
    "    MAT_FILE = 'Original/data.mat'\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, 'Simulated Images')\n",
    "    \n",
    "    \n",
    "    # Extracting Bead Images.--------------------\n",
    "    # Getiting shape of first image since all will be the same.\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], BEAD_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    bead_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting Reference Images.--------------------\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], REF_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    ref_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting MATLAB data--------------------\n",
    "    # Extracting force in X-direction.\n",
    "    DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], MAT_FILE)\n",
    "    data = read_mat(DATA_PATH)\n",
    "    temp = data['force_x'].shape\n",
    "    force_x = np.zeros((len(sorted_dirs), *data['force_x'].shape))\n",
    "    force_y = np.zeros((len(sorted_dirs), *data['force_y'].shape))\n",
    "    \n",
    "    images = np.zeros((len(sorted_dirs), np.ndim(first_image), *data['force_x'].shape), dtype=first_image.dtype)\n",
    "    forces = np.zeros((len(sorted_dirs), np.ndim(data['force_x']), *image_shape), dtype=first_image.dtype)\n",
    "                      \n",
    "    for idx, dir in tqdm(enumerate(sorted_dirs)):\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, BEAD_IMAGE_FILE)) as img:\n",
    "            bead_images[idx] = np.array(img)\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, REF_IMAGE_FILE)) as img:\n",
    "            ref_images[idx] = np.array(img)\n",
    "        images[idx] = np.stack((ref_images[idx], bead_images[idx]))\n",
    "        DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, dir, MAT_FILE)\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_x[idx] = data['force_x']\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_y[idx] = data['force_y']\n",
    "        forces[idx] = np.stack((force_x[idx], force_y[idx]))\n",
    "       \n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")\n",
    "\n",
    "    np.save('bead_images.npy', bead_images)\n",
    "    np.save('ref_images.npy', ref_images)\n",
    "    np.save('force_x.npy', force_x)\n",
    "    np.save('force_y.npy', force_y)\n",
    "    np.save('images.npy', images)\n",
    "    np.save('forces.npy', forces)\n",
    "\n",
    "else:\n",
    "    print(\"Loading files\")\n",
    "    bead_images = np.load('bead_images.npy')\n",
    "    ref_images = np.load('ref_images.npy')\n",
    "    force_x = np.load('force_x.npy')\n",
    "    force_y = np.load('force_y.npy')\n",
    "    images = np.load('images.npy')\n",
    "    forces = np.load('forces.npy')\n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884a64-359c-40f7-a08c-9903f9bcebf6",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964cc895-c744-41ee-9d49-ddcce7e111f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeforeAfterDataset(Dataset):\n",
    "    def __init__(self, images, forces, transform=None):\n",
    "        self.images = images  # 3D numpy array (N, H, W)\n",
    "        self.forces = forces   # 3D numpy array (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        force = self.forces[idx]\n",
    "        \n",
    "        # Convert to torch tensors and add channel dimension\n",
    "        image = torch.from_numpy(image).float()\n",
    "        force = torch.from_numpy(force).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            force = self.transform(force)\n",
    "        \n",
    "        return image, force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9894d-d45c-4182-a071-f53f9213c37f",
   "metadata": {},
   "source": [
    "# Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02eab5-8111-47d5-b930-4fced0042902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee24cd3-2881-4e69-87fd-4c579464675b",
   "metadata": {},
   "source": [
    "# Building basic U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f57ca-0970-4021-a47b-ee70976b2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 256x256x2\n",
    "        self.e11 = nn.Conv2d(2, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # # input: 32x32x512\n",
    "        # self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        # self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        # self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        # xp4 = self.pool4(xe42)\n",
    "\n",
    "        # xe51 = relu(self.e51(xp4))\n",
    "        # xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        # xu1 = self.upconv1(xe52)\n",
    "        # xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        # xd11 = relu(self.d11(xu11))\n",
    "        # xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        # xu2 = self.upconv2(xd12)\n",
    "        xu2 = self.upconv2(xe42)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        # xu3 = self.upconv3(xe32)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c6ce2-aacb-4db7-9b0f-6e2a7adccbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviationOfTractionMagnitudeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeviationOfTractionMagnitudeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        target_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "\n",
    "        print(target_magnitude)\n",
    "        \n",
    "        # Calculate relative error (deviation)\n",
    "        deviation = (pred_magnitude - target_magnitude)\n",
    "\n",
    "        # Take absolute value\n",
    "        abs_deviation = torch.abs(deviation)\n",
    "        # print(f\"deviation: {(pred_magnitude - target_magnitude) / (target_magnitude)}\")\n",
    "        # print(f\"Deviation with small: {(pred_magnitude - target_magnitude) / (target_magnitude + 1e-8)}\")\n",
    "        # Mean over all dimensions except batch\n",
    "        mean_deviation = torch.mean(abs_deviation, dim=(1, 2))\n",
    "        \n",
    "        # Return mean over batch\n",
    "        return torch.mean(mean_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28895-dde2-43d1-b90e-b253804f417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDTMLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8, alpha=1.0):\n",
    "        super(ModifiedDTMLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # Weight for the false positive term\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Modified Deviation of Traction Magnitude loss.\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        assert outputs.shape[1] == 2, \"Second dimension must be 2 for x and y components\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # Create a mask for non-zero true magnitudes\n",
    "        non_zero_mask = (true_magnitude > self.epsilon)\n",
    "        \n",
    "        # Calculate relative error (deviation) for non-zero true magnitudes\n",
    "        deviation = torch.where(non_zero_mask,\n",
    "                                (pred_magnitude - true_magnitude) / (true_magnitude + self.epsilon),\n",
    "                                torch.zeros_like(pred_magnitude))\n",
    "        \n",
    "        # Calculate mean absolute deviation for non-zero true magnitudes\n",
    "        mean_deviation = torch.sum(torch.abs(deviation) * non_zero_mask.float()) / (non_zero_mask.sum() + self.epsilon)\n",
    "        \n",
    "        # Penalize false positives (predicted force where there should be none)\n",
    "        false_positive_penalty = torch.mean(pred_magnitude * (~non_zero_mask).float())\n",
    "        \n",
    "        # Combine the losses\n",
    "        total_loss = mean_deviation + self.alpha * false_positive_penalty\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608ecc3-1d90-4345-a4f4-ecdb6525a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5, epsilon=1e-8):\n",
    "        super(CombinedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.epsilon = epsilon\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # Create a mask for non-zero true magnitudes\n",
    "        non_zero_mask = (true_magnitude > self.epsilon)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = torch.where(non_zero_mask,\n",
    "                                (pred_magnitude - true_magnitude) / (true_magnitude + self.epsilon),\n",
    "                                torch.zeros_like(pred_magnitude))\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0e40e-fe72-4a03-b82c-927866fa5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModifiedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5):\n",
    "        super(CombinedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = pred_magnitude - true_magnitude\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e651297-cf17-4e0d-b2e9-2d1560ee6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_clear_gpu():\n",
    "    # Check if MPS (Metal Performance Shaders) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Get GPU memory usage\n",
    "        gpu_memory = psutil.virtual_memory().percent\n",
    "        \n",
    "        print(f\"GPU Memory Usage: {gpu_memory}%\")\n",
    "        \n",
    "        # You can adjust this threshold as needed\n",
    "        if gpu_memory > 40:  # If GPU is more than 90% full\n",
    "            print(\"GPU memory is nearly full. Clearing cache...\")\n",
    "            \n",
    "            # Clear PyTorch cache\n",
    "            torch.mps.empty_cache()\n",
    "            \n",
    "            # Run garbage collector\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"Cache cleared and garbage collected.\")\n",
    "        else:\n",
    "            print(\"GPU memory is not full.\")\n",
    "    else:\n",
    "        print(\"MPS (GPU) is not available on this system.\")\n",
    "\n",
    "# Run the function\n",
    "check_and_clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03995bd4-e9e2-44b3-ba5e-14068c690ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    full_dataset = BeforeAfterDataset(images[:500, :, :, :], forces[:500, :, :, :])\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=False)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Run the function\n",
    "    check_and_clear_gpu()\n",
    "    model = UNet(n_class = 2).to(device)\n",
    "    # criterion = nn.MSELoss()\n",
    "    # criterion = DeviationOfTractionMagnitudeLoss()\n",
    "    # criterion = ModifiedDTMLoss()\n",
    "    criterion = CombinedTractionLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # Initialize metrics\n",
    "    # ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "    # psnr = PeakSignalNoiseRatio().to(device)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"Epoch: {epoch}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for train, train_labels in tqdm(train_loader):\n",
    "            train, train_labels = train.to(device), train_labels.to(device)\n",
    "            # print(f\"Train shape: {train.shape}, Train labels: {train_labels.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Forward call starting\")\n",
    "            outputs = model(train)\n",
    "            # print(\"Calculating loss now\")\n",
    "            # print(f\"Shape of outputs: {outputs.shape}\")\n",
    "            # print(f\"Shape of labels: {train_labels.shape}\")\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            # print(\"Backward Propagation now\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for test, test_labels in tqdm(test_loader):\n",
    "            test, test_labels = test.to(device), test_labels.to(device)\n",
    "            outputs = model(test)\n",
    "            loss = criterion(outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1764fc-2218-4f02-88ee-633dd4b7ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is stored in a variable called 'forces'\n",
    "# forces.shape should be (2, 256, 256)\n",
    "\n",
    "x = np.arange(0, 256)\n",
    "y = np.arange(0, 256)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Downsample for clarity if needed\n",
    "step = 3  # Adjust this to change density of arrows\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "           outputs[1, 0, ::step, ::step].cpu(), outputs[1, 1, ::step, ::step].cpu())\n",
    "plt.title('Reconstructed Force Field Visualization')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a146a1-f1e1-4d07-bcff-7dfa46d545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is stored in a variable called 'forces'\n",
    "# forces.shape should be (2, 256, 256)\n",
    "\n",
    "x = np.arange(0, 256)\n",
    "y = np.arange(0, 256)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Downsample for clarity if needed\n",
    "step = 3  # Adjust this to change density of arrows\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "           forces[500, 0, ::step, ::step], forces[500, 1, ::step, ::step])\n",
    "plt.title('Original Force Field Visualization')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b898f9-a1f7-4b05-9871-051b5cb86b70",
   "metadata": {},
   "source": [
    "# Unet + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d5e1eb7f-8494-4946-bb6b-244cb5dfdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModifiedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5):\n",
    "        super(CombinedModifiedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = pred_magnitude - true_magnitude\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "711aff65-5f68-4d08-aaee-b62fd11b1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        print(f\"DoubleConv(after): {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "12a1dcd2-23b7-4fe6-9f0d-6a9dcb6ea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [DoubleConv(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for idx, block in enumerate(self.enc_blocks):\n",
    "            print(f'\\x1b[32mEncoder(before) {idx+1}:\\x1b[0m\\n{x.shape}')\n",
    "            x = block(x)\n",
    "            print(\"Saving feature(before)\")\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "            print(f\"Max Pool (after): {x.shape}\")\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28658e92-71d4-48bf-b8d2-e8df85e06e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, dim):\n",
    "        super(AxialAttention, self).__init__()\n",
    "        self.height_attn = nn.MultiheadAttention(embed_dim = in_channels, num_heads = 8, batch_first = True)\n",
    "        self.width_attn = nn.MultiheadAttention(embed_dim = in_channels, num_heads = 8, batch_first = True)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # print(f\"Before permutation: {x.shape}\")\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # print(f\"After permutation: {x.shape}\")\n",
    "\n",
    "        # Height attention\n",
    "        x_h = x.reshape(b*w, h, c)\n",
    "        # print(f\"Height Attention (before): {x_h.shape}\")\n",
    "        x_h, _ = self.height_attn(x_h, x_h, x_h)\n",
    "        # print(f\"Height Attention (after): {x_h.shape}\")\n",
    "        x_h = x_h.reshape(b, w, h, c)\n",
    "\n",
    "        # Width attention\n",
    "        x_w = x.permute(0, 2, 1, 3).reshape(b*h, w, c)\n",
    "        x_w, _ = self.width_attn(x_w, x_w, x_w)\n",
    "        x_w = x_w.reshape(b, h, w, c).permute(0, 2, 1, 3)\n",
    "\n",
    "        x = x_h + x_w\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "137d4c44-9e05-4b7d-8e20-5edcf867d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [nn.ConvTranspose2d(channels[i], channels[i+1], kernel_size=2, stride=2) \n",
    "             for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.axial_attentions = nn.ModuleList(\n",
    "            [AxialAttention(channels[i], dim=2) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [DoubleConv(channels[i]+channels[i-1], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i, feature in enumerate(encoder_features[::-1]):\n",
    "            print(f\"\\x1b[32mDecoder (before):\\x1b[0m\\n{x.shape}\")\n",
    "            x = self.upconvs[i](x)\n",
    "            print(f\"Upconvolution(after): {x.shape}\")\n",
    "            enc_ftrs = self.axial_attentions[i](feature)\n",
    "            print(f\"Axial Attention (after): {enc_ftrs.shape}\")\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            print(f\"Concatenation: {x.shape}\")\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bf801211-b131-45e8-b82d-b44b307eaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetWithAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNetWithAxialAttention, self).__init__()\n",
    "        self.encoder_channels = [in_channels, 64, 64, 128]\n",
    "        self.decoder_channels = [128, 64, 64, 64]\n",
    "        \n",
    "        self.encoder = Encoder(self.encoder_channels)\n",
    "        self.decoder = Decoder(self.decoder_channels)\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, encoder_features = self.encoder(x)\n",
    "        print(f\"Finished Encoder(after): {len(encoder_features)}\")\n",
    "        decoder_output = self.decoder(x, encoder_features)\n",
    "        return self.final_conv(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "459b3b9a-e724-45d9-8982-e60f37f68212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bad9404ae649b2b628b9731db9787a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m \n",
      " torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m \n",
      " torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m \n",
      " torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 192, 3, 3], expected input[1, 128, 128, 128] to have 192 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"Forward call starting\")\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(\"Calculating loss now\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(f\"Shape of outputs: {outputs.shape}\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(f\"Shape of labels: {train_labels.shape}\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, train_labels)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[166], line 14\u001b[0m, in \u001b[0;36mUNetWithAxialAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m x, encoder_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Encoder(after): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(encoder_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(decoder_output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[165], line 24\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, encoder_features)\u001b[0m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, enc_ftrs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[152], line 14\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoubleConv(after): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 192, 3, 3], expected input[1, 128, 128, 128] to have 192 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "full_dataset = BeforeAfterDataset(images[:10, :, :, :], forces[:10, :, :, :])\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Run the function\n",
    "# check_and_clear_gpu()\n",
    "\n",
    "# Example usage\n",
    "model = UNetWithAxialAttention(in_channels=2, num_classes=2).to(device)\n",
    "\n",
    "criterion = CombinedModifiedTractionLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    # print(f\"Epoch: {epoch}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for train, train_labels in tqdm(train_loader):\n",
    "        train, train_labels = train.to(device), train_labels.to(device)\n",
    "        print(f\"Train shape: {train.shape}, Train labels: {train_labels.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Forward call starting\")\n",
    "        outputs = model(train)\n",
    "        # print(\"Calculating loss now\")\n",
    "        # print(f\"Shape of outputs: {outputs.shape}\")\n",
    "        # print(f\"Shape of labels: {train_labels.shape}\")\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        # print(\"Backward Propagation now\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / BATCH_SIZE\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss/Epoch: {avg_train_loss:.4f}\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for test, test_labels in tqdm(test_loader):\n",
    "        test, test_labels = test.to(device), test_labels.to(device)\n",
    "        outputs = model(test)\n",
    "        loss = criterion(outputs, test_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Calculate average metrics\n",
    "\n",
    "avg_test_loss = test_loss / BATCH_SIZE\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14dd6e3a-76c4-439c-ad9c-b71d753b10b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3,4,5]\n",
    "\n",
    "for i in temp[::-1]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66074a3-1822-43fc-bcca-d03e61897c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d06824-bdef-4cda-b809-88c4f9101dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
