{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964d5cf0-2d8e-45bf-99ad-61caf9c19a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matlab.engine\n",
    "import h5py\n",
    "import pymatreader\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "# from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2335f-1ca9-40c9-a72a-05d5b1cd1625",
   "metadata": {},
   "source": [
    "# Running MATLAB code to generate simulated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5d06ea-f4f8-4db4-bea7-f03625f16b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED_IMAGES_FOLDER = 'Simulated Images/'\n",
    "# ROOT_DIR = os.getcwd()\n",
    "# SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "# if os.listdir(SIMULATED_IMAGES_PATH):\n",
    "#     # Start MATLAB engine\n",
    "#     eng = matlab.engine.start_matlab()\n",
    "#     # Specify the root folder containing your MATLAB functions\n",
    "#     # root_folder = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Image Simulation Code/'\n",
    "#     # Generate path string including all subfolders\n",
    "#     # path_string = eng.genpath(root_folder)\n",
    "    \n",
    "#     # Add the generated path to MATLAB search path\n",
    "#     # eng.addpath(path_string, nargout=0)\n",
    "#     nExp = 20;\n",
    "#     force_start = 200.0;\n",
    "#     force_step = 200.0;\n",
    "#     force_stop = 4000.0;\n",
    "#     distance_start = 2.0;\n",
    "#     distance_step = 2.0;\n",
    "#     distance_stop = 20.0;\n",
    "#     storePath = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Simulated Images/'\n",
    "    \n",
    "#     eng.CSPrunTestSingleForcePython(nExp, \n",
    "#                                     force_start, \n",
    "#                                     force_step, \n",
    "#                                     force_stop, \n",
    "#                                     distance_start, \n",
    "#                                     distance_step,\n",
    "#                                     distance_stop,\n",
    "#                                     storePath, \n",
    "#                                     nargout = 0)\n",
    "    \n",
    "#     eng.quit()\n",
    "# else:\n",
    "#     print(\"Simulated Images already present, either delete previous data or provide new path to empty directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50d8fb-ab7d-4975-9890-37528a88bb46",
   "metadata": {},
   "source": [
    "# Get all directories in 'Simulated Images' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a8d62d-af99-4b75-9c7f-18a56bf2331f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_directories(directory_path):\n",
    "    # Get all directories in the specified path\n",
    "    directories = [d for d in Path(directory_path).iterdir() if d.is_dir()]\n",
    "    \n",
    "    # Sort directories by modification time\n",
    "    sorted_directories = sorted(directories, key=lambda x: x.stat().st_mtime, reverse=False)\n",
    "    \n",
    "    return sorted_directories\n",
    "\n",
    "# Example usage\n",
    "SIMULATED_IMAGES_FOLDER = 'Simulated Images'\n",
    "ROOT_DIR = os.getcwd()\n",
    "SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "sorted_dirs = get_sorted_directories(SIMULATED_IMAGES_PATH)\n",
    "\n",
    "# for directory in sorted_dirs:\n",
    "#     print(f\"{directory.name}: {os.path.getmtime(directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb34a6b-e5ec-4913-a682-1276d515c753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc6c4d-8dbe-4eb3-84b7-190b693ed83e",
   "metadata": {},
   "source": [
    "# Extract all images, and force vectors from all directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed187e5b-050e-4286-8b85-8ff30557930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files\n"
     ]
    }
   ],
   "source": [
    "np_files = ['bead_images.npy', 'ref_images.npy', 'force_x.npy', 'force_y.npy', 'images.npy', 'forces.npy']\n",
    "all_present = True\n",
    "\n",
    "for file in np_files:\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), file)):\n",
    "        print(f\"{file} does not exist\")\n",
    "        all_present = False\n",
    "        \n",
    "if not all_present:\n",
    "    REF_IMAGE_FILE = 'Reference/img1ref.tif'\n",
    "    BEAD_IMAGE_FILE = 'Beads/img2bead.tif'\n",
    "    MAT_FILE = 'Original/data.mat'\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, 'Simulated Images')\n",
    "    \n",
    "    \n",
    "    # Extracting Bead Images.--------------------\n",
    "    # Getiting shape of first image since all will be the same.\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], BEAD_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    bead_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting Reference Images.--------------------\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], REF_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    ref_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting MATLAB data--------------------\n",
    "    # Extracting force in X-direction.\n",
    "    DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], MAT_FILE)\n",
    "    data = read_mat(DATA_PATH)\n",
    "    temp = data['force_x'].shape\n",
    "    force_x = np.zeros((len(sorted_dirs), *data['force_x'].shape))\n",
    "    force_y = np.zeros((len(sorted_dirs), *data['force_y'].shape))\n",
    "    \n",
    "    images = np.zeros((len(sorted_dirs), np.ndim(first_image), *data['force_x'].shape), dtype=first_image.dtype)\n",
    "    forces = np.zeros((len(sorted_dirs), np.ndim(data['force_x']), *image_shape), dtype=first_image.dtype)\n",
    "                      \n",
    "    for idx, dir in tqdm(enumerate(sorted_dirs)):\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, BEAD_IMAGE_FILE)) as img:\n",
    "            bead_images[idx] = np.array(img)\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, REF_IMAGE_FILE)) as img:\n",
    "            ref_images[idx] = np.array(img)\n",
    "        images[idx] = np.stack((ref_images[idx], bead_images[idx]))\n",
    "        DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, dir, MAT_FILE)\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_x[idx] = data['force_x']\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_y[idx] = data['force_y']\n",
    "        forces[idx] = np.stack((force_x[idx], force_y[idx]))\n",
    "       \n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")\n",
    "\n",
    "    np.save('bead_images.npy', bead_images)\n",
    "    np.save('ref_images.npy', ref_images)\n",
    "    np.save('force_x.npy', force_x)\n",
    "    np.save('force_y.npy', force_y)\n",
    "    np.save('images.npy', images)\n",
    "    np.save('forces.npy', forces)\n",
    "\n",
    "else:\n",
    "    print(\"Loading files\")\n",
    "    bead_images = np.load('bead_images.npy')\n",
    "    ref_images = np.load('ref_images.npy')\n",
    "    force_x = np.load('force_x.npy')\n",
    "    force_y = np.load('force_y.npy')\n",
    "    images = np.load('images.npy')\n",
    "    forces = np.load('forces.npy')\n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff964ce-9c07-458c-97a0-656ab6b15397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4000, 256, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((ref_images, bead_images)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374f426-6300-4dcd-ad78-0ea0be9421f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e884a64-359c-40f7-a08c-9903f9bcebf6",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964cc895-c744-41ee-9d49-ddcce7e111f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeforeAfterDataset(Dataset):\n",
    "    def __init__(self, images, forces, transform=None):\n",
    "        self.images = np.stack(  # 3D numpy array (N, H, W)\n",
    "        self.forces = forces   # 3D numpy array (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        force = self.forces[idx]\n",
    "        \n",
    "        # Convert to torch tensors and add channel dimension\n",
    "        image = torch.from_numpy(image).float()\n",
    "        force = torch.from_numpy(force).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            force = self.transform(force)\n",
    "        \n",
    "        return image, force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9894d-d45c-4182-a071-f53f9213c37f",
   "metadata": {},
   "source": [
    "# Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c02eab5-8111-47d5-b930-4fced0042902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee24cd3-2881-4e69-87fd-4c579464675b",
   "metadata": {},
   "source": [
    "# Building basic U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288f57ca-0970-4021-a47b-ee70976b2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 256x256x2\n",
    "        self.e11 = nn.Conv2d(2, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # # input: 68x68x256\n",
    "        # self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        # self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # # input: 32x32x512\n",
    "        # self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        # self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        # self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        # self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        # xp3 = self.pool3(xe32)\n",
    "\n",
    "        # xe41 = relu(self.e41(xp3))\n",
    "        # xe42 = relu(self.e42(xe41))\n",
    "        # xp4 = self.pool4(xe42)\n",
    "\n",
    "        # xe51 = relu(self.e51(xp4))\n",
    "        # xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        # xu1 = self.upconv1(xe52)\n",
    "        # xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        # xd11 = relu(self.d11(xu11))\n",
    "        # xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        # xu2 = self.upconv2(xd12)\n",
    "        # xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        # xd21 = relu(self.d21(xu22))\n",
    "        # xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        # xu3 = self.upconv3(xd22)\n",
    "        xu3 = self.upconv3(xe32)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42c6ce2-aacb-4db7-9b0f-6e2a7adccbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviationOfTractionMagnitudeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeviationOfTractionMagnitudeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs**2 + pred_y**2)\n",
    "        target_magnitude = torch.sqrt(target_x**2 + target_y**2)\n",
    "        \n",
    "        # Calculate relative error (deviation)\n",
    "        deviation = (pred_magnitude - target_magnitude) / (target_magnitude + 1e-8)\n",
    "        \n",
    "        # Take absolute value and mean\n",
    "        mean_deviation = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        return mean_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e651297-cf17-4e0d-b2e9-2d1560ee6a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage: 65.7%\n",
      "GPU memory is nearly full. Clearing cache...\n",
      "Cache cleared and garbage collected.\n"
     ]
    }
   ],
   "source": [
    "def check_and_clear_gpu():\n",
    "    # Check if MPS (Metal Performance Shaders) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Get GPU memory usage\n",
    "        gpu_memory = psutil.virtual_memory().percent\n",
    "        \n",
    "        print(f\"GPU Memory Usage: {gpu_memory}%\")\n",
    "        \n",
    "        # You can adjust this threshold as needed\n",
    "        if gpu_memory > 40:  # If GPU is more than 90% full\n",
    "            print(\"GPU memory is nearly full. Clearing cache...\")\n",
    "            \n",
    "            # Clear PyTorch cache\n",
    "            torch.mps.empty_cache()\n",
    "            \n",
    "            # Run garbage collector\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"Cache cleared and garbage collected.\")\n",
    "        else:\n",
    "            print(\"GPU memory is not full.\")\n",
    "    else:\n",
    "        print(\"MPS (GPU) is not available on this system.\")\n",
    "\n",
    "# Run the function\n",
    "check_and_clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03995bd4-e9e2-44b3-ba5e-14068c690ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage: 64.3%\n",
      "GPU memory is nearly full. Clearing cache...\n",
      "Cache cleared and garbage collected.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9177a88b4149fa8b1d6efb3a8e06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([64, 256, 256]), Train labels: torch.Size([64, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 2, 3, 3], expected input[1, 64, 256, 256] to have 2 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(\"Forward call starting\")\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(\"Calculating loss now\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of outputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     xe11 \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me11\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     57\u001b[0m     xe12 \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me12(xe11))\n\u001b[1;32m     58\u001b[0m     xp1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(xe12)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 2, 3, 3], expected input[1, 64, 256, 256] to have 2 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    full_dataset = BeforeAfterDataset(images, forces)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Run the function\n",
    "    check_and_clear_gpu()\n",
    "    model = UNet(n_class = 2).to(device)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = DeviationOfTractionMagnitudeLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # Initialize metrics\n",
    "    # ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "    # psnr = PeakSignalNoiseRatio().to(device)\n",
    "    \n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"Epoch: {epoch}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for train, train_labels in tqdm(train_loader):\n",
    "            train, train_labels = train.to(device), train_labels.to(device)\n",
    "            print(f\"Train shape: {train.shape}, Train labels: {train_labels.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Forward call starting\")\n",
    "            outputs = model(train)\n",
    "            # print(\"Calculating loss now\")\n",
    "            print(f\"Shape of outputs: {outputs.shape}\")\n",
    "            print(f\"Shape of labels: {train_labels.shape}\")\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            # print(\"Backward Propagation now\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for test, test_labels in tqdm(test_loader):\n",
    "        test, test_labels = test.to(device), test_labels.to(device)\n",
    "        outputs = model(test)\n",
    "        loss = criterion(outputs, test_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_train_loss = train_loss / len(train_loader)\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a73ec1-2105-4c44-8f12-49b5d09aeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_force_field(x_forces, y_forces, title):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     x, y = np.meshgrid(np.arange(256), np.arange(256))\n",
    "#     plt.quiver(x, y, x_forces, y_forces, scale=50, width=0.002)\n",
    "#     plt.title(title)\n",
    "#     plt.axis('equal')\n",
    "\n",
    "# # Assuming pred_forces and true_forces are your predicted and true force tensors\n",
    "# pred_x, pred_y = pred_forces[0, 0].cpu().numpy(), pred_forces[0, 1].cpu().numpy()\n",
    "# true_x, true_y = true_forces[0, 0].cpu().numpy(), true_forces[0, 1].cpu().numpy()\n",
    "\n",
    "# plot_force_field(pred_x, pred_y, \"Predicted Forces\")\n",
    "# plot_force_field(true_x, true_y, \"True Forces\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
