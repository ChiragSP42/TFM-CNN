{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964d5cf0-2d8e-45bf-99ad-61caf9c19a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matlab.engine\n",
    "import h5py\n",
    "import pymatreader\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "# from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94570f-2c73-40da-8ec5-e897c40e5244",
   "metadata": {},
   "source": [
    "# Initializing Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe151be4-6336-4e57-8e58-9299a5b24761",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project = 'Unet_Axial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2335f-1ca9-40c9-a72a-05d5b1cd1625",
   "metadata": {},
   "source": [
    "# Running MATLAB code to generate simulated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5d06ea-f4f8-4db4-bea7-f03625f16b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED_IMAGES_FOLDER = 'Simulated Images/'\n",
    "# ROOT_DIR = os.getcwd()\n",
    "# SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "# if os.listdir(SIMULATED_IMAGES_PATH):\n",
    "#     # Start MATLAB engine\n",
    "#     eng = matlab.engine.start_matlab()\n",
    "#     # Specify the root folder containing your MATLAB functions\n",
    "#     # root_folder = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Image Simulation Code/'\n",
    "#     # Generate path string including all subfolders\n",
    "#     # path_string = eng.genpath(root_folder)\n",
    "    \n",
    "#     # Add the generated path to MATLAB search path\n",
    "#     # eng.addpath(path_string, nargout=0)\n",
    "#     nExp = 20;\n",
    "#     force_start = 200.0;\n",
    "#     force_step = 200.0;\n",
    "#     force_stop = 4000.0;\n",
    "#     distance_start = 2.0;\n",
    "#     distance_step = 2.0;\n",
    "#     distance_stop = 20.0;\n",
    "#     storePath = '/Users/ChiragSP/MTU/Fall 2024/BE5870/Final Project/Simulated Images/'\n",
    "    \n",
    "#     eng.CSPrunTestSingleForcePython(nExp, \n",
    "#                                     force_start, \n",
    "#                                     force_step, \n",
    "#                                     force_stop, \n",
    "#                                     distance_start, \n",
    "#                                     distance_step,\n",
    "#                                     distance_stop,\n",
    "#                                     storePath, \n",
    "#                                     nargout = 0)\n",
    "    \n",
    "#     eng.quit()\n",
    "# else:\n",
    "#     print(\"Simulated Images already present, either delete previous data or provide new path to empty directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50d8fb-ab7d-4975-9890-37528a88bb46",
   "metadata": {},
   "source": [
    "# Get all directories in 'Simulated Images' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a8d62d-af99-4b75-9c7f-18a56bf2331f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_sorted_directories(directory_path):\n",
    "#     # Get all directories in the specified path\n",
    "#     directories = [d for d in Path(directory_path).iterdir() if d.is_dir()]\n",
    "    \n",
    "#     # Sort directories by modification time\n",
    "#     sorted_directories = sorted(directories, key=lambda x: x.stat().st_mtime, reverse=False)\n",
    "    \n",
    "#     return sorted_directories\n",
    "\n",
    "# # Example usage\n",
    "# SIMULATED_IMAGES_FOLDER = 'Simulated Images'\n",
    "# ROOT_DIR = os.getcwd()\n",
    "# SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, SIMULATED_IMAGES_FOLDER)\n",
    "# sorted_dirs = get_sorted_directories(SIMULATED_IMAGES_PATH)\n",
    "\n",
    "# # for directory in sorted_dirs:\n",
    "# #     print(f\"{directory.name}: {os.path.getmtime(directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc6c4d-8dbe-4eb3-84b7-190b693ed83e",
   "metadata": {},
   "source": [
    "# Extract all images, and force vectors from all directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed187e5b-050e-4286-8b85-8ff30557930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files\n",
      "Final dimensions:\n",
      "Number of reference images: 4000 of shape: (256, 256)\n",
      "Number of bead images: 4000 of shape: (256, 256)\n",
      "Force in X-direction: (4000, 256, 256)\n",
      "Force in Y-direction: (4000, 256, 256)\n",
      "Dimension of images: (4000, 2, 256, 256)\n",
      "Dimension of forces: (4000, 2, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "np_files = ['bead_images.npy', 'ref_images.npy', 'force_x.npy', 'force_y.npy', 'images.npy', 'forces.npy']\n",
    "all_present = True\n",
    "\n",
    "for file in np_files:\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), file)):\n",
    "        print(f\"{file} does not exist\")\n",
    "        all_present = False\n",
    "        \n",
    "if not all_present:\n",
    "    REF_IMAGE_FILE = 'Reference/img1ref.tif'\n",
    "    BEAD_IMAGE_FILE = 'Beads/img2bead.tif'\n",
    "    MAT_FILE = 'Original/data.mat'\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    SIMULATED_IMAGES_PATH = os.path.join(ROOT_DIR, 'Simulated Images')\n",
    "    \n",
    "    \n",
    "    # Extracting Bead Images.--------------------\n",
    "    # Getiting shape of first image since all will be the same.\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], BEAD_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    bead_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting Reference Images.--------------------\n",
    "    with Image.open(os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], REF_IMAGE_FILE)) as img:\n",
    "        first_image = np.array(img)\n",
    "        image_shape = first_image.shape\n",
    "    ref_images = np.zeros((len(sorted_dirs), *image_shape), dtype=first_image.dtype)\n",
    "    \n",
    "    # Extracting MATLAB data--------------------\n",
    "    # Extracting force in X-direction.\n",
    "    DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, sorted_dirs[0], MAT_FILE)\n",
    "    data = read_mat(DATA_PATH)\n",
    "    temp = data['force_x'].shape\n",
    "    force_x = np.zeros((len(sorted_dirs), *data['force_x'].shape))\n",
    "    force_y = np.zeros((len(sorted_dirs), *data['force_y'].shape))\n",
    "    \n",
    "    images = np.zeros((len(sorted_dirs), np.ndim(first_image), *data['force_x'].shape), dtype=first_image.dtype)\n",
    "    forces = np.zeros((len(sorted_dirs), np.ndim(data['force_x']), *image_shape), dtype=first_image.dtype)\n",
    "                      \n",
    "    for idx, dir in tqdm(enumerate(sorted_dirs)):\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, BEAD_IMAGE_FILE)) as img:\n",
    "            bead_images[idx] = np.array(img)\n",
    "        with Image.open(os.path.join(SIMULATED_IMAGES_PATH, dir, REF_IMAGE_FILE)) as img:\n",
    "            ref_images[idx] = np.array(img)\n",
    "        images[idx] = np.stack((ref_images[idx], bead_images[idx]))\n",
    "        DATA_PATH = os.path.join(SIMULATED_IMAGES_PATH, dir, MAT_FILE)\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_x[idx] = data['force_x']\n",
    "        data = read_mat(DATA_PATH)\n",
    "        force_y[idx] = data['force_y']\n",
    "        forces[idx] = np.stack((force_x[idx], force_y[idx]))\n",
    "       \n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")\n",
    "\n",
    "    np.save('bead_images.npy', bead_images)\n",
    "    np.save('ref_images.npy', ref_images)\n",
    "    np.save('force_x.npy', force_x)\n",
    "    np.save('force_y.npy', force_y)\n",
    "    np.save('images.npy', images)\n",
    "    np.save('forces.npy', forces)\n",
    "\n",
    "else:\n",
    "    print(\"Loading files\")\n",
    "    bead_images = np.load('bead_images.npy')\n",
    "    ref_images = np.load('ref_images.npy')\n",
    "    force_x = np.load('force_x.npy')\n",
    "    force_y = np.load('force_y.npy')\n",
    "    images = np.load('images.npy')\n",
    "    forces = np.load('forces.npy')\n",
    "    print(f\"Final dimensions:\\nNumber of reference images: {ref_images.shape[0]} of shape: {ref_images.shape[1:]}\")\n",
    "    print(f\"Number of bead images: {bead_images.shape[0]} of shape: {bead_images.shape[1:]}\")\n",
    "    print(f\"Force in X-direction: {force_x.shape}\")\n",
    "    print(f\"Force in Y-direction: {force_y.shape}\")\n",
    "    print(f\"Dimension of images: {images.shape}\")\n",
    "    print(f\"Dimension of forces: {forces.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884a64-359c-40f7-a08c-9903f9bcebf6",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964cc895-c744-41ee-9d49-ddcce7e111f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeforeAfterDataset(Dataset):\n",
    "    def __init__(self, images, forces, transform=None):\n",
    "        self.images = images  # 3D numpy array (N, H, W)\n",
    "        self.forces = forces   # 3D numpy array (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        force = self.forces[idx]\n",
    "        \n",
    "        # Convert to torch tensors and add channel dimension\n",
    "        image = torch.from_numpy(image).float()\n",
    "        force = torch.from_numpy(force).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            force = self.transform(force)\n",
    "        \n",
    "        return image, force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28895-dde2-43d1-b90e-b253804f417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDTMLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8, alpha=1.0):\n",
    "        super(ModifiedDTMLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # Weight for the false positive term\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Modified Deviation of Traction Magnitude loss.\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        assert outputs.shape[1] == 2, \"Second dimension must be 2 for x and y components\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # Create a mask for non-zero true magnitudes\n",
    "        non_zero_mask = (true_magnitude > self.epsilon)\n",
    "        \n",
    "        # Calculate relative error (deviation) for non-zero true magnitudes\n",
    "        deviation = torch.where(non_zero_mask,\n",
    "                                (pred_magnitude - true_magnitude) / (true_magnitude + self.epsilon),\n",
    "                                torch.zeros_like(pred_magnitude))\n",
    "        \n",
    "        # Calculate mean absolute deviation for non-zero true magnitudes\n",
    "        mean_deviation = torch.sum(torch.abs(deviation) * non_zero_mask.float()) / (non_zero_mask.sum() + self.epsilon)\n",
    "        \n",
    "        # Penalize false positives (predicted force where there should be none)\n",
    "        false_positive_penalty = torch.mean(pred_magnitude * (~non_zero_mask).float())\n",
    "        \n",
    "        # Combine the losses\n",
    "        total_loss = mean_deviation + self.alpha * false_positive_penalty\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608ecc3-1d90-4345-a4f4-ecdb6525a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5, epsilon=1e-8):\n",
    "        super(CombinedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.epsilon = epsilon\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # Create a mask for non-zero true magnitudes\n",
    "        non_zero_mask = (true_magnitude > self.epsilon)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = torch.where(non_zero_mask,\n",
    "                                (pred_magnitude - true_magnitude) / (true_magnitude + self.epsilon),\n",
    "                                torch.zeros_like(pred_magnitude))\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0e40e-fe72-4a03-b82c-927866fa5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModifiedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5):\n",
    "        super(CombinedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = pred_magnitude - true_magnitude\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1764fc-2218-4f02-88ee-633dd4b7ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is stored in a variable called 'forces'\n",
    "# forces.shape should be (2, 256, 256)\n",
    "\n",
    "x = np.arange(0, 256)\n",
    "y = np.arange(0, 256)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Downsample for clarity if needed\n",
    "step = 3  # Adjust this to change density of arrows\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "           outputs[1, 0, ::step, ::step].cpu(), outputs[1, 1, ::step, ::step].cpu())\n",
    "plt.title('Reconstructed Force Field Visualization')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a146a1-f1e1-4d07-bcff-7dfa46d545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is stored in a variable called 'forces'\n",
    "# forces.shape should be (2, 256, 256)\n",
    "\n",
    "x = np.arange(0, 256)\n",
    "y = np.arange(0, 256)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Downsample for clarity if needed\n",
    "step = 3  # Adjust this to change density of arrows\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "           forces[500, 0, ::step, ::step], forces[500, 1, ::step, ::step])\n",
    "plt.title('Original Force Field Visualization')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b898f9-a1f7-4b05-9871-051b5cb86b70",
   "metadata": {},
   "source": [
    "# Unet + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d5e1eb7f-8494-4946-bb6b-244cb5dfdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModifiedTractionLoss(nn.Module):\n",
    "    def __init__(self, dtm_weight=0.5, mse_weight=0.5):\n",
    "        super(CombinedModifiedTractionLoss, self).__init__()\n",
    "        self.dtm_weight = dtm_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        Compute the Combined Traction Loss (DTM + MSE).\n",
    "        \n",
    "        Args:\n",
    "        outputs (torch.Tensor): Predicted traction forces, shape (batch, 2, height, width)\n",
    "        labels (torch.Tensor): True traction forces, shape (batch, 2, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "        assert outputs.shape == labels.shape, \"Output and label shapes must match\"\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        pred_magnitude = torch.sqrt(outputs[:, 0, :, :]**2 + outputs[:, 1, :, :]**2)\n",
    "        true_magnitude = torch.sqrt(labels[:, 0, :, :]**2 + labels[:, 1, :, :]**2)\n",
    "        \n",
    "        # DTM calculation\n",
    "        deviation = pred_magnitude - true_magnitude\n",
    "        \n",
    "        dtm_loss = torch.mean(torch.abs(deviation))\n",
    "        \n",
    "        # MSE calculation\n",
    "        mse = self.mse_loss(outputs, labels)\n",
    "        mse_loss = torch.mean(mse)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.dtm_weight * dtm_loss + self.mse_weight * mse_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "711aff65-5f68-4d08-aaee-b62fd11b1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # print(f\"DoubleConv(after): {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "12a1dcd2-23b7-4fe6-9f0d-6a9dcb6ea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [DoubleConv(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for idx, block in enumerate(self.enc_blocks):\n",
    "            # print(f'\\x1b[32mEncoder(before) {idx+1}:\\x1b[0m\\n{x.shape}')\n",
    "            x = block(x)\n",
    "            # print(\"Saving feature(before)\")\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "            # print(f\"Max Pool (after): {x.shape}\")\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "28658e92-71d4-48bf-b8d2-e8df85e06e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, dim):\n",
    "        super(AxialAttention, self).__init__()\n",
    "        self.height_attn = nn.MultiheadAttention(embed_dim = in_channels, num_heads = 8, batch_first = True)\n",
    "        self.width_attn = nn.MultiheadAttention(embed_dim = in_channels, num_heads = 8, batch_first = True)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # print(f\"Before permutation: {x.shape}\")\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        # print(f\"After permutation: {x.shape}\")\n",
    "\n",
    "        # Height attention\n",
    "        x_h = x.reshape(b*w, h, c)\n",
    "        # print(f\"Height Attention (before): {x_h.shape}\")\n",
    "        x_h, _ = self.height_attn(x_h, x_h, x_h)\n",
    "        # print(f\"Height Attention (after): {x_h.shape}\")\n",
    "        x_h = x_h.reshape(b, w, h, c)\n",
    "\n",
    "        # Width attention\n",
    "        x_w = x.permute(0, 2, 1, 3).reshape(b*h, w, c)\n",
    "        x_w, _ = self.width_attn(x_w, x_w, x_w)\n",
    "        x_w = x_w.reshape(b, h, w, c).permute(0, 2, 1, 3)\n",
    "\n",
    "        x = x_h + x_w\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "137d4c44-9e05-4b7d-8e20-5edcf867d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [nn.ConvTranspose2d(channels[i], channels[i+1], kernel_size=2, stride=2) \n",
    "             for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.axial_attentions = nn.ModuleList(\n",
    "            [AxialAttention(channels[i], dim=2) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [DoubleConv(channels[i]+channels[i+1], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i, feature in enumerate(encoder_features[::-1]):\n",
    "            # print(f\"\\x1b[32mDecoder {i+1}(before):\\x1b[0m\\n{x.shape}\")\n",
    "            x = self.upconvs[i](x)\n",
    "            # print(f\"Upconvolution(after): {x.shape}\")\n",
    "            enc_ftrs = self.axial_attentions[i](feature)\n",
    "            # print(f\"Axial Attention (after): {enc_ftrs.shape}\")\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            # print(f\"Concatenation: {x.shape}\")\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bf801211-b131-45e8-b82d-b44b307eaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetWithAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNetWithAxialAttention, self).__init__()\n",
    "        self.encoder_channels = [in_channels, 64, 64, 128]\n",
    "        self.decoder_channels = [128, 64, 64, 64]\n",
    "        \n",
    "        self.encoder = Encoder(self.encoder_channels)\n",
    "        self.decoder = Decoder(self.decoder_channels)\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, encoder_features = self.encoder(x)\n",
    "        # print(f\"Finished Encoder(after): {len(encoder_features)}\")\n",
    "        decoder_output = self.decoder(x, encoder_features)\n",
    "        return self.final_conv(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "459b3b9a-e724-45d9-8982-e60f37f68212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba3af6e8ffb43c9ae0812547e5b493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Epoch [1/2]\n",
      "Train Loss/Epoch: 399.3216\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec6d2a96ba4184880e15ab2d08728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Train shape: torch.Size([1, 2, 256, 256]), Train labels: torch.Size([1, 2, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Epoch [2/2]\n",
      "Train Loss/Epoch: 394.0288\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f415d840a388429fa4c3a747795591b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "\u001b[32mEncoder(before) 1:\u001b[0m\n",
      "torch.Size([1, 2, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mEncoder(before) 2:\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mEncoder(before) 3:\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 128, 64, 64])\n",
      "Saving feature(before)\n",
      "Max Pool (after): torch.Size([1, 128, 32, 32])\n",
      "Finished Encoder(after): 3\n",
      "\u001b[32mDecoder 1(before):\u001b[0m\n",
      "torch.Size([1, 128, 32, 32])\n",
      "Upconvolution(after): torch.Size([1, 64, 64, 64])\n",
      "Axial Attention (after): torch.Size([1, 128, 64, 64])\n",
      "Concatenation: torch.Size([1, 192, 64, 64])\n",
      "DoubleConv(after): torch.Size([1, 64, 64, 64])\n",
      "\u001b[32mDecoder 2(before):\u001b[0m\n",
      "torch.Size([1, 64, 64, 64])\n",
      "Upconvolution(after): torch.Size([1, 64, 128, 128])\n",
      "Axial Attention (after): torch.Size([1, 64, 128, 128])\n",
      "Concatenation: torch.Size([1, 128, 128, 128])\n",
      "DoubleConv(after): torch.Size([1, 64, 128, 128])\n",
      "\u001b[32mDecoder 3(before):\u001b[0m\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Upconvolution(after): torch.Size([1, 64, 256, 256])\n",
      "Axial Attention (after): torch.Size([1, 64, 256, 256])\n",
      "Concatenation: torch.Size([1, 128, 256, 256])\n",
      "DoubleConv(after): torch.Size([1, 64, 256, 256])\n",
      "Test Loss: 29.9922\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "full_dataset = BeforeAfterDataset(images[:100, :, :, :], forces[:100, :, :, :])\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"\\x1b[31mCUDA AVAILABLE\\x1b[0m\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\x1b[31mCUDA NOT AVAILABLE\\x1b[0m\")\n",
    "\n",
    "# Example usage\n",
    "model = UNetWithAxialAttention(in_channels=2, num_classes=2).to(device)\n",
    "\n",
    "criterion = CombinedModifiedTractionLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    # print(f\"Epoch: {epoch}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for train, train_labels in tqdm(train_loader):\n",
    "        train, train_labels = train.to(device), train_labels.to(device)\n",
    "        # print(f\"Train shape: {train.shape}, Train labels: {train_labels.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Forward call starting\")\n",
    "        outputs = model(train)\n",
    "        # print(\"Calculating loss now\")\n",
    "        # print(f\"Shape of outputs: {outputs.shape}\")\n",
    "        # print(f\"Shape of labels: {train_labels.shape}\")\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        # print(\"Backward Propagation now\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / BATCH_SIZE\n",
    "    wandb.log({\"Epoch Train loss\": avg_train_loss})\n",
    "    if avg_train_loss < best_loss:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss/Epoch: {avg_train_loss:.4f}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd6e3a-76c4-439c-ad9c-b71d753b10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"\\x1b[31mCUDA AVAILABLE\\x1b[0m\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\x1b[31mCUDA NOT AVAILABLE\\x1b[0m\")\n",
    "\n",
    "# Example usage\n",
    "eval_model = UNetWithAxialAttention(in_channels=2, num_classes=2).to(device)\n",
    "\n",
    "eval_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "eval_model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for test, test_labels in tqdm(test_loader):\n",
    "        test, test_labels = test.to(device), test_labels.to(device)\n",
    "        outputs = eval_model(test)\n",
    "        loss = criterion(outputs, test_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Calculate average metrics\n",
    "\n",
    "avg_test_loss = test_loss / BATCH_SIZE\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66074a3-1822-43fc-bcca-d03e61897c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d06824-bdef-4cda-b809-88c4f9101dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
